{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en_core_web_lg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_0 = nlp(u'Donald Trump visited at the government headquarters in France today.')\n",
    "sent_1 = nlp(u'Emmanuel Jean-Michel Frédéric Macron is a French politician serving as President of France and ex officio Co-Prince of Andorra since 14 May 2017.')\n",
    "sent_2 = nlp(u\"He studied philosophy at Paris Nanterre University, completed a Master's of Public Affairs at Sciences Po, and graduated from the École nationale d'administration (ÉNA) in 2004.\")\n",
    "sent_3 = nlp(u'He worked at the Inspectorate General of Finances, and later became an investment banker at Rothschild & Cie Banque.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Donald Trump PERSON\n",
      "France GPE\n",
      "today DATE\n"
     ]
    }
   ],
   "source": [
    "for token in sent_0.ents:\n",
    "    print(token.text,token.label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Emmanuel PERSON\n",
      "Jean PERSON\n",
      "- PERSON\n",
      "Michel PERSON\n",
      "Frédéric PERSON\n",
      "Macron PERSON\n",
      "is \n",
      "a \n",
      "French NORP\n",
      "politician \n",
      "serving \n",
      "as \n",
      "President \n",
      "of \n",
      "France GPE\n",
      "and \n",
      "ex \n",
      "officio \n",
      "Co \n",
      "- \n",
      "Prince \n",
      "of \n",
      "Andorra \n",
      "since \n",
      "14 DATE\n",
      "May DATE\n",
      "2017 DATE\n",
      ". \n"
     ]
    }
   ],
   "source": [
    "for token in sent_1:\n",
    "    print(token.text,token.ent_type_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paris Nanterre University ORG\n",
      "Sciences Po ORG\n",
      "École nationale d'administration ORG\n",
      "2004 DATE\n"
     ]
    }
   ],
   "source": [
    "for token in sent_2.ents:\n",
    "    print(token.text,token.label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the Inspectorate General of Finances ORG\n",
      "Rothschild & Cie Banque ORG\n"
     ]
    }
   ],
   "source": [
    "for token in sent_3.ents:\n",
    "    print(token.text,token.label_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import plac\n",
    "import random\n",
    "from pathlib import Path\n",
    "import spacy\n",
    "# training data\n",
    "TRAIN_DATA = [\n",
    "('Who is Shaka Khan?', {\n",
    "'entities': [(7, 17, 'PERSON')]\n",
    "}),\n",
    "('I like London and Berlin.', {\n",
    "'entities': [(7, 13, 'LOC'), (18, 24, 'LOC')]\n",
    "})\n",
    "]\n",
    "We've set up our basic imports and our training examples. A friendly reminder that these\n",
    "are far too few examples for any serious training to happen, and that is merely a\n",
    "representative example.\n",
    "\n",
    "\n",
    "@plac.annotations(\n",
    "model=(\"Model name. Defaults to blank 'en' model.\", \"option\", \"m\",\n",
    "str),\n",
    "output_dir=(\"Optional output directory\", \"option\", \"o\", Path),\n",
    "n_iter=(\"Number of training iterations\", \"option\", \"n\", int))\n",
    "def main(model=None, output_dir=None, n_iter=100):\n",
    "\"\"\"Load the model, set up the pipeline and train the\n",
    "entity recognizer.\"\"\"\n",
    "if model is not None:\n",
    "nlp = spacy.load(model) # load existing spaCy model\n",
    "print(\"Loaded model '%s'\" % model)\n",
    "else:\n",
    "nlp = spacy.blank('en') # create blank Language class\n",
    "print(\"Created blank 'en' model\")\n",
    "\n",
    "\n",
    "\n",
    "We've set up annotations for where our model will be saved, as well as the number of\n",
    "iterations. Our model is loaded, and we have now created a blank model.\n",
    "# create the built-in pipeline components and add them to the pipeline\n",
    "# nlp.create_pipe works for built-ins that are registered with spaCy\n",
    "if 'ner' not in nlp.pipe_names:\n",
    "ner = nlp.create_pipe('ner')\n",
    "nlp.add_pipe(ner, last=True)\n",
    "# otherwise, get it so we can add labels\n",
    "else:\n",
    "ner = nlp.get_pipe('ner')\n",
    "# add labels\n",
    "for _, annotations in TRAIN_DATA:\n",
    "for ent in annotations.get('entities'):\n",
    "ner.add_label(ent[2])\n",
    "# get names of other pipes to disable them during training\n",
    "other_pipes = [pipe for pipe in nlp.pipe_names if pipe != 'ner']\n",
    "with nlp.disable_pipes(*other_pipes): # only train NER\n",
    "optimizer = nlp.begin_training()\n",
    "for itn in range(n_iter):\n",
    "random.shuffle(TRAIN_DATA)\n",
    "losses = {}\n",
    "for text, annotations in TRAIN_DATA:\n",
    "nlp.update(\n",
    "[text], # batch of texts\n",
    "[annotations], # batch of annotations\n",
    "drop=0.5, # dropout-make it harder to memorise data\n",
    "sgd=optimizer, # callable to update weights\n",
    "losses=losses)\n",
    "print(losses)\n",
    "\n",
    "\n",
    "We've set up annotations for where our model will be saved, as well as the number of\n",
    "iterations. Our model is loaded, and we have now created a blank model.\n",
    "# create the built-in pipeline components and add them to the pipeline\n",
    "# nlp.create_pipe works for built-ins that are registered with spaCy\n",
    "if 'ner' not in nlp.pipe_names:\n",
    "ner = nlp.create_pipe('ner')\n",
    "nlp.add_pipe(ner, last=True)\n",
    "# otherwise, get it so we can add labels\n",
    "else:\n",
    "ner = nlp.get_pipe('ner')\n",
    "# add labels\n",
    "for _, annotations in TRAIN_DATA:\n",
    "for ent in annotations.get('entities'):\n",
    "ner.add_label(ent[2])\n",
    "# get names of other pipes to disable them during training\n",
    "other_pipes = [pipe for pipe in nlp.pipe_names if pipe != 'ner']\n",
    "with nlp.disable_pipes(*other_pipes): # only train NER\n",
    "optimizer = nlp.begin_training()\n",
    "for itn in range(n_iter):\n",
    "random.shuffle(TRAIN_DATA)\n",
    "losses = {}\n",
    "for text, annotations in TRAIN_DATA:\n",
    "nlp.update(\n",
    "[text], # batch of texts\n",
    "[annotations], # batch of annotations\n",
    "drop=0.5, # dropout-make it harder to memorise data\n",
    "sgd=optimizer, # callable to update weights\n",
    "losses=losses)\n",
    "print(losses)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "We notice here that it follows the exact same training principles as the POS-tagger. We start\n",
    "by adding the ner label to the pipeline, and then disabling all the other components of the\n",
    "pipe so that we only train/update the NER-tagger. The training itself is straightforward,\n",
    "and the nlp.update() method abstracts everything for us, letting spaCy deal with the\n",
    "actual machine learning and heavy lifting.\n",
    "# test the trained model\n",
    "for text, _ in TRAIN_DATA:\n",
    "doc = nlp(text)\n",
    "print('Entities', [(ent.text, ent.label_) for ent in doc.ents])\n",
    "print('Tokens', [(t.text, t.ent_type_, t.ent_iob) for t in doc])\n",
    "# save model to output directory\n",
    "if output_dir is not None:\n",
    "output_dir = Path(output_dir)\n",
    "if not output_dir.exists():\n",
    "output_dir.mkdir()\n",
    "nlp.to_disk(output_dir)\n",
    "print(\"Saved model to\", output_dir)\n",
    "# test the saved model\n",
    "print(\"Loading from\", output_dir)\n",
    "nlp2 = spacy.load(output_dir)\n",
    "for text, _ in TRAIN_DATA:\n",
    "doc = nlp2(text)\n",
    "print('Entities', [(ent.text, ent.label_) for ent in doc.ents])\n",
    "print('Tokens', [(t.text, t.ent_type_, t.ent_iob) for t in\n",
    "doc])\n",
    "if __name__ == '__main__':\n",
    "plac.call(main)\n",
    "Soon after our training is done, we test our model and then save it to the directory\n",
    "specified. If we run the file without any errors, we should expect the following output:\n",
    "Entities [('Shaka Khan', 'PERSON')]\n",
    "Tokens [('Who', '', 2), ('is', '', 2), ('Shaka', 'PERSON', 3),\n",
    "('Khan', 'PERSON', 1), ('?', '', 2)]\n",
    "Entities [('London', 'LOC'), ('Berlin', 'LOC')]\n",
    "Tokens [('I', '', 2), ('like', '', 2), ('London', 'LOC', 3),\n",
    "('and', '', 2), ('Berlin', 'LOC', 3), ('.', '', 2)]\n",
    "Let's now look at adding a new class to a model. The principle remains the same here; we\n",
    "load the model, disable the pipes we won't be updating, add the new label, and then loop\n",
    "over the examples and update them. Again, exactly like the old example, don't expect the\n",
    "trained model to do any wonders – we don't have enough training examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plac\n",
    "import random\n",
    "from pathlib import Path\n",
    "import spacy\n",
    "\n",
    "# new entity label\n",
    "LABEL = 'ANIMAL'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_DATA = [\n",
    "(\"Horses are too tall and they pretend to care about your feelings\", {\n",
    "'entities': [(0, 6, 'ANIMAL')]\n",
    "}),\n",
    "(\"Do they bite?\", {\n",
    "'entities': []\n",
    "}),\n",
    "(\"horses are too tall and they pretend to care about your feelings\", {\n",
    "'entities': [(0, 6, 'ANIMAL')]\n",
    "}),\n",
    "(\"horses pretend to care about your feelings\", {\n",
    "'entities': [(0, 6, 'ANIMAL')]\n",
    "}),\n",
    "(\"they pretend to care about your feelings, those horses\", {\n",
    "'entities': [(48, 54, 'ANIMAL')]\n",
    "}),\n",
    "(\"horses?\", {\n",
    "'entities': [(0, 6, 'ANIMAL')]\n",
    "})\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "@plac.annotations(\n",
    "model=(\"en_core_web_lg\", \"option\", \"m\",\n",
    "str),\n",
    "new_model_name=(\"AnimalEntity\", \"option\", \"nm\", str),\n",
    "output_dir=(\".\", \"option\", \"o\", Path),\n",
    "n_iter=(10, \"option\", \"n\", int))\n",
    "def main(model=None, new_model_name='animal', output_dir=None, n_iter=20):\n",
    "    \"\"\"Set up the pipeline and entity recognizer, and train the new entity.\"\"\"\n",
    "    if model is not None:\n",
    "        nlp = spacy.load(model)  # load existing spaCy model\n",
    "        print(\"Loaded model '%s'\" % model)\n",
    "    else:\n",
    "        nlp = spacy.blank('en')  # create blank Language class\n",
    "        print(\"Created blank 'en' model\")\n",
    "        # Add entity recognizer to model if it's not in the pipeline\n",
    "        # nlp.create_pipe works for built-ins that are registered with spaCy\n",
    "    if 'ner' not in nlp.pipe_names:\n",
    "        ner = nlp.create_pipe('ner')\n",
    "        nlp.add_pipe(ner)\n",
    "        # otherwise, get it, so we can add labels to it\n",
    "    else:\n",
    "        ner = nlp.get_pipe('ner')\n",
    "        ner.add_label(LABEL)\n",
    "        if model is None:\n",
    "            optimizer = nlp.begin_training()\n",
    "        else:\n",
    "            optimizer = nlp.entity.create_optimizer()\n",
    "            other_pipes = [pipe for pipe in nlp.pipe_names if pipe!='ner']\n",
    "            with nlp.disable_pipes(*other_pipes):\n",
    "                for itn in range(n_iter):\n",
    "                    random.shuffle(TRAIN_DATA)\n",
    "                    losses = {}\n",
    "                    for text,annotations in TRAIN_DATA:\n",
    "                        nlp.update([text],[annotations],sgd=optimizer,drop=0.35,losses=losses)\n",
    "                    print(losses)\n",
    "                    \n",
    "    #test the model\n",
    "    test_text = \"Do you like horses?\"\n",
    "    doc = nlp(test_text)\n",
    "    print(\"Entities are\")\n",
    "    for ent in doc.ents:\n",
    "        print(ent.label_,ent.text)\n",
    "\n",
    "    # save model to output directory\n",
    "    if output_dir is not None:\n",
    "        output_dir = Path(output_dir)\n",
    "        if not output_dir.exists():\n",
    "            output_dir.mkdir()\n",
    "        nlp.meta['name'] = new_model_name # rename model\n",
    "        nlp.to_disk(output_dir)\n",
    "        print(\"Saved model to\", output_dir)\n",
    "        # test the saved model\n",
    "        print(\"Loading from\", output_dir)\n",
    "        nlp2 = spacy.load(output_dir)\n",
    "        doc2 = nlp2(test_text)\n",
    "        for ent in doc2.ents:\n",
    "            print(ent.label_, ent.text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] [-m None] [-nm animal] [-o None] [-n 20]\n",
      "ipykernel_launcher.py: error: unrecognized arguments: --ip=127.0.0.1 --stdin=9004 --control=9002 --hb=9001 --Session.signature_scheme=\"hmac-sha256\" --Session.key=b\"3a6167eb-21f6-4184-9c12-54860e1b72fd\" --shell=9003 --transport=\"tcp\" --iopub=9005 --f=C:\\Users\\z003va8b\\AppData\\Local\\Temp\\tmp-242040NM0vgboFkRW.json\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[1;31mSystemExit\u001b[0m\u001b[1;31m:\u001b[0m 2\n"
     ]
    }
   ],
   "source": [
    "plac.call(main)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp.create_optimizer()\n",
    "nlp.update()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "8c6fbc1fa9f8e22016cc4c5b1d6cee697708750d816f9ec70f6447d44ab62434"
  },
  "kernelspec": {
   "display_name": "Python 3.10.2 64-bit ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
